{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 060623\n",
    "# simple pre-processing for one dataset which fits to Doug's data model\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd    \n",
    "import matplotlib.pyplot as plt\n",
    "import openpyxl\n",
    "import xml.etree.ElementTree as ET\n",
    "import pyodbc\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send the pre-processed data to SQL\n",
    "# connect to SQL server\n",
    "def connect2SQL():\n",
    "    print(\"Connection attempt to SQL\")\n",
    "    conn = pyodbc.connect('Driver={SQL Server};'\n",
    "                        'Server=ELW12DDB01;' #ELW12DDB01 US-KR5ENN0\\SQLEXPRESS\n",
    "                        'Database=AI_PROCESS_DEV_D;' #AI_PROCESS_DEV_D practice\n",
    "                        'Trusted_Connection=yes;')\n",
    "    cursor = conn.cursor()\n",
    "    print(\"Connected to SQL successfully\")\n",
    "    return conn, cursor\n",
    "\n",
    "## insert data into parameter_D\n",
    "def Insert2PARAMETER_D(conn, cursor, df_param):\n",
    "    print(\"Insertion attempt to PARAMETER_D\")\n",
    "    for row in df_param.itertuples(index=False):\n",
    "        try:\n",
    "            cursor.execute('''\n",
    "                INSERT INTO PARAMETER_D (PARAMETER_NM)\n",
    "                VALUES (?)\n",
    "                ''',\n",
    "                row.PARAMETER_NM\n",
    "            )\n",
    "            conn.commit()\n",
    "        except pyodbc.IntegrityError as e:\n",
    "            print(\"IntegrityError: {}\".format(e))\n",
    "            conn.rollback()\n",
    "    \n",
    "    print(\"Insertion attempt to PARAMETER_D is complete\")\n",
    "\n",
    "## insert data into SPECIES_D\n",
    "def Insert2SPECIES_D(conn, cursor, df_species):\n",
    "    print(\"Insertion attempt to SPECIES_D\")\n",
    "    for row in df_species.itertuples(index=False):\n",
    "        print(row)\n",
    "        try:\n",
    "            cursor.execute('''\n",
    "                INSERT INTO SPECIES_D ( SPECIES_NM)\n",
    "                VALUES ( ?)\n",
    "                ''',\n",
    "                row.SPECIES_NM\n",
    "            )\n",
    "            conn.commit()\n",
    "        except pyodbc.IntegrityError as e:\n",
    "            print(\"IntegrityError: {}\".format(e))\n",
    "            conn.rollback()\n",
    "    print(\"Insertion attempt to SPECIES_D is complete\")\n",
    "\n",
    "## insert data into batch_run_D\n",
    "def Insert2BATCH_RUN_D(conn, cursor, df_batchrun):\n",
    "    print(\"Insertion attempt to BATCH_RUN_D\")\n",
    "    for row in df_batchrun.itertuples(index=False):\n",
    "        try:\n",
    "            cursor.execute('''\n",
    "                INSERT INTO BATCH_RUN_D (BATCH_RUN_ID, BATCH_RUN_DATE, DESCRIPTION, PURPOSE, BACKGROUND, CONCLUSIONS, NEXT_STEPS, USER_NM, PROJECT_NM, FILE_NM)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                ''',\n",
    "                row.BATCH_RUN_ID,\n",
    "                row.BATCH_RUN_DATE,\n",
    "                row.DESCRIPTION,\n",
    "                row.PURPOSE,\n",
    "                row.BACKGROUND,\n",
    "                row.CONCLUSIONS,\n",
    "                row.NEXT_STEPS,\n",
    "                row.USER_NM,\n",
    "                row.PROJECT_NM,\n",
    "                row.FILE_NM\n",
    "            )\n",
    "        except pyodbc.IntegrityError as e:\n",
    "            print(\"IntegrityError: {}\".format(e))\n",
    "            conn.rollback()\n",
    "    conn.commit()\n",
    "    print(\"Insertion attempt to BATCH_RUN_D is complete\")\n",
    "\n",
    "def Insert2RESULT_F(conn, cursor, df_result):\n",
    "    print(\"Insertion attempt to RESULT_F\")\n",
    "    idx = 0\n",
    "    for row in df_result.itertuples(index=False):\n",
    "        idx+=1\n",
    "        if idx%1000 ==0:\n",
    "            print(\"reading RESULT_F... row={}\".format(idx))\n",
    "\n",
    "        try:\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO RESULT_F (BATCH_RUN_KEY, PARAMETER_KEY, SPECIES_KEY, DATA_SOURCE, DATA_SEQUENCE, MASTER_VAL, STRING_VAL, NUMERIC_VAL, DATETIME_VAL, DATA_TYPE, UNIT_CD, RELATIVE_TIME, RELATIVE_TIME_S)\n",
    "                VALUES ((SELECT BATCH_RUN_KEY FROM BATCH_RUN_D WHERE BATCH_RUN_ID = '{}'), \n",
    "                (SELECT PARAMETER_KEY FROM PARAMETER_D WHERE PARAMETER_NM = '{}'),\n",
    "                (SELECT isnull(SPECIES_KEY,'-999') FROM SPECIES_D WHERE SPECIES_NM = '{}'),\n",
    "                '{}', {}, {}, {}, {}, '{}', '{}', '{}', '{}', {})\n",
    "                \"\"\".format(\n",
    "                row.BATCH_RUN_ID,\n",
    "                row.PARAMETER_NM,\n",
    "                row.SPECIES_NM,\n",
    "                row.DATA_SOURCE,\n",
    "                row.DATA_SEQUENCE,\n",
    "                row.MASTER_VAL,\n",
    "                row.STRING_VAL,\n",
    "                row.NUMERIC_VAL,\n",
    "                row.DATETIME_VAL,\n",
    "                row.DATA_TYPE,\n",
    "                row.UNIT_CD,\n",
    "                row.RELATIVE_TIME,\n",
    "                row.RELATIVE_TIME_S)\n",
    "            )\n",
    "        except pyodbc.IntegrityError as e:\n",
    "            print(\"IntegrityError: {}\".format(e))\n",
    "            conn.rollback()\n",
    "    conn.commit()\n",
    "    print(\"Insertion attempt to RESULT_F is complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BATCH_RUN(df,file_path):\n",
    "    # IMPORT BATCHRUN TABLE FROM SQL\n",
    "    \n",
    "    # query = 'SELECT BATCH_RUN_KEY FROM BATCH_RUN_D order by BATCH_RUN_KEY DESC'\n",
    "    # df_batchrun_key = pd.read_sql(query,conn)\n",
    "      \n",
    "    # if df_batchrun_key.shape[0]==0:\n",
    "    #     value_BATCH_RUN_KEY=1\n",
    "    # else:\n",
    "    #     value_BATCH_RUN_KEY=df_batchrun_key[\"BATCH_RUN_KEY\"][0]+1\n",
    "    \n",
    "    value_BATCH_RUN_DATE=df.at[2,'Abs. Time (UTC-04 : 00)']\n",
    "    value_DESCRIPTION=''\n",
    "    value_PURPOSE=''\n",
    "    value_BACKGROUND=''\n",
    "    value_CONCLUSIONS=''\n",
    "    value_NEXT_STEPS=''\n",
    "\n",
    "    file_segment = file_path.split('\\\\')\n",
    "    idx_experiment=file_segment.index('Experiments')\n",
    "    value_USER_NM = file_segment[idx_experiment+1]\n",
    "    value_PROJECT_NM = file_segment[idx_experiment+2]\n",
    "    value_BATCH_RUN_ID = file_segment[idx_experiment+4].split('.xlsx')[0]\n",
    "    value_FILE_NM=file_path\n",
    "\n",
    "    new_rows=[]\n",
    "    new_rows.append([value_BATCH_RUN_ID, value_BATCH_RUN_DATE, value_DESCRIPTION, value_PURPOSE, value_BACKGROUND, value_CONCLUSIONS, value_NEXT_STEPS, value_USER_NM, value_PROJECT_NM, value_FILE_NM])\n",
    "\n",
    "    # Create Batch_Run_D Table\n",
    "    batchrun_column=['BATCH_RUN_ID','BATCH_RUN_DATE','DESCRIPTION','PURPOSE','BACKGROUND','CONCLUSIONS','NEXT_STEPS','USER_NM', 'PROJECT_NM','FILE_NM']\n",
    "    df_batchrun=pd.DataFrame(columns=batchrun_column)\n",
    "    new_df = pd.DataFrame(new_rows, columns=df_batchrun.columns)\n",
    "    df_batchrun = pd.concat([df_batchrun, new_df], ignore_index=True)\n",
    "\n",
    "    # Insert to SQL\n",
    "    conn, cursor= connect2SQL()\n",
    "    Insert2BATCH_RUN_D(conn, cursor, df_batchrun)\n",
    "    return df_batchrun\n",
    "\n",
    "\n",
    "def PARAMETER_D(df):\n",
    "    # 1. import the current parameter table from sql server\n",
    "    # 2. compare it with the column names in this experiment. \n",
    "    # 3. if new parameter is introduced, insert the new parameter into the sql server\n",
    "    # IMPORT PARAMETER TABLE FROM SQL\n",
    "    conn, cursor= connect2SQL()\n",
    "    query = 'SELECT PARAMETER_NM FROM PARAMETER_D order by PARAMETER_KEY'\n",
    "    df_param = pd.read_sql(query,conn)\n",
    "\n",
    "    # change the turbidity column name: iC Vision Experiment\\E-178359-028\\Turbidity IA -> Turbidity IA \n",
    "    for parameter in list(df.columns):\n",
    "        if 'Turbidity' in parameter:\n",
    "            df=df.rename(columns={parameter:'Turbidity'})\n",
    "\n",
    "    # check if there are new element in parameter\n",
    "    new_row = []\n",
    "    for parameter in list(df.columns):\n",
    "        if parameter not in list(df_param[\"PARAMETER_NM\"]):\n",
    "            if ('TotalMass' in parameter) or ('MassFlow' in parameter) or ('TotalVolume' in parameter) or ('Temperature' in parameter):\n",
    "                pass\n",
    "            else:\n",
    "                print(\"new parameter: {}\".format(parameter))\n",
    "                new_row = [parameter]\n",
    "                new_df = pd.DataFrame(new_row, columns = df_param.columns)\n",
    "                # insert the new parameter\n",
    "                Insert2PARAMETER_D(conn, cursor, new_df)\n",
    "                # Update df_param\n",
    "                df_param = pd.concat([df_param, new_df],ignore_index=True)\n",
    "\n",
    "    conn.close()\n",
    "    return df_param, df\n",
    "        \n",
    "def SPECIES_D(df):\n",
    "    # Get the list of species\n",
    "    species_name = []\n",
    "    for parameter in list(df.columns):\n",
    "        if ('TotalMass' in parameter) or ('MassFlow' in parameter) or ('TotalVolume' in parameter) or ('Temperature' in parameter):\n",
    "            species_name.append(parameter.split('.')[0])\n",
    "    species_name = list(set(species_name))\n",
    "    print(\"Species in this experiment: {}\".format(species_name))\n",
    "\n",
    "    # IMPORT SPECIES TABLE FROM SQL\n",
    "    conn, cursor= connect2SQL()\n",
    "    query = 'SELECT SPECIES_NM FROM SPECIES_D order by SPECIES_KEY'\n",
    "    df_species =pd.read_sql(query,conn)\n",
    "\n",
    "    # check if there are new element in species\n",
    "    for species in species_name:\n",
    "        if species not in list(df_species[\"SPECIES_NM\"]):\n",
    "            new_row = [species]\n",
    "            new_df = pd.DataFrame(new_row, columns = df_species.columns)\n",
    "            # insert the new parameter\n",
    "            Insert2SPECIES_D(conn, cursor, new_df)\n",
    "            # Update df_species\n",
    "            df_species = pd.concat([df_species, new_df],ignore_index=True)\n",
    "    conn.close()\n",
    "    return df_species\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RESULT_F(df, df_batchrun, data_source):\n",
    "    row_interval = 15\n",
    "    index = 0\n",
    "\n",
    "    # Create RESULT_F Table\n",
    "    result_column = ['BATCH_RUN_ID','PARAMETER_NM','SPECIES_NM','DATA_SOURCE','DATA_SEQUENCE','MASTER_VAL','STRING_VAL','NUMERIC_VAL','DATETIME_VAL','DATA_TYPE','UNIT_CD','RELATIVE_TIME','RELATIVE_TIME_S'] # 12 cols\n",
    "    df_result=pd.DataFrame(columns=result_column)\n",
    "\n",
    "    for row in range(1, df.shape[0], row_interval):\n",
    "        \n",
    "        datetime_val = df.at[row, 'Abs. Time (UTC-04 : 00)']\n",
    "        relative_time = df.at[row, 'Rel. Time']\n",
    "        relative_time_s = df.at[row, 'Rel. Time (in s)']\n",
    "        \n",
    "        new_rows = []\n",
    "        current_row = df.iloc[row] # pd.Series\n",
    "        for col in range(4, df.shape[1]):\n",
    "            colname = current_row.index[col] # column name\n",
    "            \n",
    "            # batch_run_ID\n",
    "            value_BATCH_RUN_ID = df_batchrun[\"BATCH_RUN_ID\"][0]\n",
    "\n",
    "            # parameter_NM and species_NM\n",
    "            if ('TotalMass' in colname) or ('MassFlow' in colname) or ('TotalVolume' in colname) or ('Temperature' in colname):\n",
    "                value_SPECIES_NM = colname.split('.')[0]\n",
    "                value_PARAMETER_NM = 'Species.'+colname.split('.')[1]\n",
    "            else:\n",
    "                value_SPECIES_NM = None\n",
    "                value_PARAMETER_NM = colname\n",
    "\n",
    "            # data sequence\n",
    "            index += 1\n",
    "            value_DATA_SEQUENCE = index\n",
    "            value_DATA_SOURCE = data_source\n",
    "            \n",
    "            # Values\n",
    "            if pd.isna(current_row[col]):\n",
    "                value_NUMERIC_VAL = 'Null'\n",
    "            else:\n",
    "                value_NUMERIC_VAL = current_row[col]\n",
    "            value_MASTER_VAL = 'Null'\n",
    "            value_STRING_VAL = 'Null'\n",
    "  \n",
    "            value_DATA_TYPE = str(type(current_row[col])).replace('\\'','')\n",
    "            value_UNIT_CD = df.at[0, colname]            \n",
    "            new_rows.append([value_BATCH_RUN_ID, value_PARAMETER_NM, value_SPECIES_NM, value_DATA_SOURCE, value_DATA_SEQUENCE, value_MASTER_VAL, value_STRING_VAL, value_NUMERIC_VAL, datetime_val, value_DATA_TYPE, value_UNIT_CD, relative_time, relative_time_s])\n",
    "\n",
    "        new_df = pd.DataFrame(new_rows, columns=df_result.columns)\n",
    "        df_result = pd.concat([df_result, new_df], ignore_index=True)\n",
    "\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search excel files under the folder: \\\\elw16picdc01\\Experiments\\johanna.strul\\XDE-521\\E-178359-025\n",
      "import start\n",
      "file path: \\\\elw16picdc01\\Experiments\\johanna.strul\\XDE-521\\E-178359-025\\E-178359-025.xlsx\n",
      "import complete\n",
      "PREPROCESSING start\n",
      "Species in this experiment: ['Cyclohexane', 'XR-521', 'butyl acetate']\n",
      "Connection attempt to SQL\n",
      "Connected to SQL successfully\n",
      "Connection attempt to SQL\n",
      "Connected to SQL successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iqr474.PHIBRED\\AppData\\Local\\Temp\\ipykernel_30264\\3978563044.py:88: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_species =pd.read_sql(query,conn)\n",
      "C:\\Users\\iqr474.PHIBRED\\AppData\\Local\\Temp\\ipykernel_30264\\3978563044.py:48: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_param = pd.read_sql(query,conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection attempt to SQL\n",
      "Connected to SQL successfully\n",
      "Insertion attempt to BATCH_RUN_D\n",
      "Insertion attempt to BATCH_RUN_D is complete\n",
      "PREPROCESSING complete\n",
      "Connection attempt to SQL\n",
      "Connected to SQL successfully\n",
      "Insertion attempt to RESULT_F\n",
      "reading RESULT_F... row=1000\n",
      "reading RESULT_F... row=2000\n",
      "reading RESULT_F... row=3000\n",
      "reading RESULT_F... row=4000\n",
      "reading RESULT_F... row=5000\n",
      "reading RESULT_F... row=6000\n",
      "reading RESULT_F... row=7000\n",
      "reading RESULT_F... row=8000\n",
      "reading RESULT_F... row=9000\n",
      "reading RESULT_F... row=10000\n",
      "reading RESULT_F... row=11000\n",
      "reading RESULT_F... row=12000\n",
      "reading RESULT_F... row=13000\n",
      "reading RESULT_F... row=14000\n",
      "reading RESULT_F... row=15000\n",
      "reading RESULT_F... row=16000\n",
      "reading RESULT_F... row=17000\n",
      "reading RESULT_F... row=18000\n",
      "reading RESULT_F... row=19000\n",
      "reading RESULT_F... row=20000\n",
      "reading RESULT_F... row=21000\n",
      "reading RESULT_F... row=22000\n",
      "reading RESULT_F... row=23000\n",
      "reading RESULT_F... row=24000\n",
      "reading RESULT_F... row=25000\n",
      "reading RESULT_F... row=26000\n",
      "reading RESULT_F... row=27000\n",
      "reading RESULT_F... row=28000\n",
      "reading RESULT_F... row=29000\n",
      "reading RESULT_F... row=30000\n",
      "reading RESULT_F... row=31000\n",
      "reading RESULT_F... row=32000\n",
      "reading RESULT_F... row=33000\n",
      "reading RESULT_F... row=34000\n",
      "reading RESULT_F... row=35000\n",
      "reading RESULT_F... row=36000\n",
      "reading RESULT_F... row=37000\n",
      "reading RESULT_F... row=38000\n",
      "reading RESULT_F... row=39000\n",
      "reading RESULT_F... row=40000\n",
      "reading RESULT_F... row=41000\n",
      "reading RESULT_F... row=42000\n",
      "reading RESULT_F... row=43000\n",
      "reading RESULT_F... row=44000\n",
      "reading RESULT_F... row=45000\n",
      "reading RESULT_F... row=46000\n",
      "reading RESULT_F... row=47000\n",
      "reading RESULT_F... row=48000\n",
      "reading RESULT_F... row=49000\n",
      "reading RESULT_F... row=50000\n",
      "reading RESULT_F... row=51000\n",
      "reading RESULT_F... row=52000\n",
      "reading RESULT_F... row=53000\n",
      "reading RESULT_F... row=54000\n",
      "reading RESULT_F... row=55000\n",
      "reading RESULT_F... row=56000\n",
      "reading RESULT_F... row=57000\n",
      "reading RESULT_F... row=58000\n",
      "reading RESULT_F... row=59000\n",
      "reading RESULT_F... row=60000\n",
      "reading RESULT_F... row=61000\n",
      "reading RESULT_F... row=62000\n",
      "reading RESULT_F... row=63000\n",
      "reading RESULT_F... row=64000\n",
      "reading RESULT_F... row=65000\n",
      "reading RESULT_F... row=66000\n",
      "reading RESULT_F... row=67000\n",
      "reading RESULT_F... row=68000\n",
      "reading RESULT_F... row=69000\n",
      "reading RESULT_F... row=70000\n",
      "reading RESULT_F... row=71000\n",
      "reading RESULT_F... row=72000\n",
      "reading RESULT_F... row=73000\n",
      "reading RESULT_F... row=74000\n",
      "reading RESULT_F... row=75000\n",
      "reading RESULT_F... row=76000\n",
      "reading RESULT_F... row=77000\n",
      "reading RESULT_F... row=78000\n",
      "reading RESULT_F... row=79000\n",
      "reading RESULT_F... row=80000\n",
      "reading RESULT_F... row=81000\n",
      "reading RESULT_F... row=82000\n",
      "reading RESULT_F... row=83000\n",
      "reading RESULT_F... row=84000\n",
      "reading RESULT_F... row=85000\n",
      "reading RESULT_F... row=86000\n",
      "reading RESULT_F... row=87000\n",
      "reading RESULT_F... row=88000\n",
      "Insertion attempt to RESULT_F is complete\n",
      "Disconnected from SQL\n",
      "search excel files under the folder: \\\\elw16picdc01\\Experiments\\johanna.strul\\XDE-521\\E-178359-026\n",
      "import start\n",
      "file path: \\\\elw16picdc01\\Experiments\\johanna.strul\\XDE-521\\E-178359-026\\E-178359-026.xlsx\n",
      "import complete\n",
      "PREPROCESSING start\n",
      "Species in this experiment: ['Cyclohexane', 'XR-521 feed', 'butyl acetate']\n",
      "Connection attempt to SQL\n",
      "Connected to SQL successfully\n",
      "Insertion attempt to SPECIES_D\n",
      "Pandas(SPECIES_NM='XR-521 feed')\n",
      "Insertion attempt to SPECIES_D is complete\n",
      "Connection attempt to SQL\n",
      "Connected to SQL successfully\n",
      "Connection attempt to SQL\n",
      "Connected to SQL successfully\n",
      "Insertion attempt to BATCH_RUN_D\n",
      "Insertion attempt to BATCH_RUN_D is complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iqr474.PHIBRED\\AppData\\Local\\Temp\\ipykernel_30264\\3978563044.py:88: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_species =pd.read_sql(query,conn)\n",
      "C:\\Users\\iqr474.PHIBRED\\AppData\\Local\\Temp\\ipykernel_30264\\3978563044.py:48: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_param = pd.read_sql(query,conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPROCESSING complete\n",
      "Connection attempt to SQL\n",
      "Connected to SQL successfully\n",
      "Insertion attempt to RESULT_F\n",
      "reading RESULT_F... row=1000\n",
      "reading RESULT_F... row=2000\n",
      "reading RESULT_F... row=3000\n",
      "reading RESULT_F... row=4000\n",
      "reading RESULT_F... row=5000\n",
      "reading RESULT_F... row=6000\n",
      "reading RESULT_F... row=7000\n",
      "reading RESULT_F... row=8000\n",
      "reading RESULT_F... row=9000\n",
      "reading RESULT_F... row=10000\n",
      "reading RESULT_F... row=11000\n",
      "reading RESULT_F... row=12000\n",
      "reading RESULT_F... row=13000\n",
      "reading RESULT_F... row=14000\n",
      "reading RESULT_F... row=15000\n",
      "reading RESULT_F... row=16000\n",
      "reading RESULT_F... row=17000\n",
      "reading RESULT_F... row=18000\n",
      "reading RESULT_F... row=19000\n",
      "reading RESULT_F... row=20000\n",
      "reading RESULT_F... row=21000\n",
      "reading RESULT_F... row=22000\n",
      "reading RESULT_F... row=23000\n",
      "reading RESULT_F... row=24000\n",
      "reading RESULT_F... row=25000\n",
      "reading RESULT_F... row=26000\n",
      "reading RESULT_F... row=27000\n",
      "reading RESULT_F... row=28000\n",
      "reading RESULT_F... row=29000\n",
      "reading RESULT_F... row=30000\n",
      "reading RESULT_F... row=31000\n",
      "reading RESULT_F... row=32000\n",
      "reading RESULT_F... row=33000\n",
      "reading RESULT_F... row=34000\n",
      "reading RESULT_F... row=35000\n",
      "reading RESULT_F... row=36000\n",
      "reading RESULT_F... row=37000\n",
      "reading RESULT_F... row=38000\n",
      "reading RESULT_F... row=39000\n",
      "reading RESULT_F... row=40000\n",
      "reading RESULT_F... row=41000\n",
      "reading RESULT_F... row=42000\n",
      "reading RESULT_F... row=43000\n",
      "reading RESULT_F... row=44000\n",
      "reading RESULT_F... row=45000\n",
      "reading RESULT_F... row=46000\n",
      "reading RESULT_F... row=47000\n",
      "reading RESULT_F... row=48000\n",
      "reading RESULT_F... row=49000\n",
      "reading RESULT_F... row=50000\n",
      "reading RESULT_F... row=51000\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "IMPORT        = True\n",
    "PREPROCESSING = True\n",
    "PUSH2SQL      = True\n",
    "\n",
    "start = 25\n",
    "end = 30\n",
    "for number in range(start, end):\n",
    "    # xlsx file search under a folder path\n",
    "    folder_path = r'\\\\elw16picdc01\\Experiments\\johanna.strul\\XDE-521\\E-178359-0'+str(number) #\\\\elw16picdc01\\Experiments\\paul.larsen\\XR-521 2021\\E-176325-070; \\\\elw16picdc01\\Experiments\\johanna.strul\\XDE-521\\E-178359-023\n",
    "    print(\"search excel files under the folder: {}\".format(folder_path))\n",
    "    # Use the glob function to search for .xlsx files in the folder\n",
    "    xlsx_files = glob.glob(folder_path + \"/*.xlsx\")\n",
    "\n",
    "    for file_path in xlsx_files:\n",
    "        if IMPORT:\n",
    "            # import data\n",
    "            print(\"import start\")\n",
    "            print(\"file path: {}\".format(file_path))\n",
    "            df = pd.read_excel(file_path)\n",
    "            data_source = 'i-Control' # need to modify later\n",
    "            print(\"import complete\")\n",
    "\n",
    "        if PREPROCESSING:\n",
    "            print(\"PREPROCESSING start\")\n",
    "            df_species = SPECIES_D(df) # update SPECIES_D\n",
    "            df_param, df = PARAMETER_D(df) # update PARAMETER_D\n",
    "            df_batchrun= BATCH_RUN(df,file_path) # update BATCH_RUN_D\n",
    "            df_result =RESULT_F(df, df_batchrun, data_source)   \n",
    "            print(\"PREPROCESSING complete\")\n",
    "\n",
    "        if PUSH2SQL:\n",
    "            conn, cursor= connect2SQL()\n",
    "            Insert2RESULT_F(conn, cursor, df_result)\n",
    "            conn.close()\n",
    "            print(\"Disconnected from SQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
